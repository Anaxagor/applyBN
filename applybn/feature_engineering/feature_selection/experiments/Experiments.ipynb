{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adfdf30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "save_directory = \"my_datasets\"\n",
    "import time\n",
    "import concurrent.futures\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25a0808d",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_df_classif=['adult', 'aloi',  'churn',   'eye',  'gesture', 'helena', 'higgs-small',  'jannis',  'otto']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ab0f1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_df_regr=['california_housing', 'microsoft','year', 'house']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590a8278",
   "metadata": {},
   "source": [
    "## Отбор признаков с помощью grow shrink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f30cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from grow_shrink import GrowShrink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31b823b",
   "metadata": {},
   "outputs": [],
   "source": [
    "CI_tests = [PartialCorrelation()]  \n",
    "p_values = [0.01, 0.05, 0.1]\n",
    "\n",
    "\n",
    "\n",
    "dataset_configs = ['0', '01', '02', '03']\n",
    "\n",
    "gs_results = []\n",
    "timeout = 10 * 60  \n",
    "\n",
    "\n",
    "def gs_algorithm_running(X_train, X_test, y_train, y_test, CI_test, pva):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    y_train_df = pd.DataFrame(y_train, columns=['target'])\n",
    "    train_df = pd.concat([X_train, y_train_df], axis=1)\n",
    "    \n",
    "    data_array = train_df.values\n",
    "    var_names = train_df.columns.tolist()\n",
    "    \n",
    "    StandardizeTransform_ = StandardizeTransform()\n",
    "    StandardizeTransform_.fit(data_array)\n",
    "    data = StandardizeTransform_.transform(data_array)\n",
    "    \n",
    "    target_var = 'target'\n",
    "    gs = GrowShrink(data=data, CI_test=ci_test)\n",
    "    selected_features = gs.run(target_var='target', pvalue_thres=pva)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    \n",
    "    return selected_features, elapsed_time\n",
    "\n",
    "\n",
    "def run_with_timeout(func, *args):\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        future = executor.submit(func, *args)\n",
    "        try:\n",
    "            result = future.result(timeout=timeout)\n",
    "        except concurrent.futures.TimeoutError:\n",
    "            return [\"max\"], \"max\"\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f96f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_results=[]\n",
    "for dataset in list_df_regr:\n",
    "    print(dataset)\n",
    "    \n",
    "   \n",
    "    for config in dataset_configs:\n",
    "        print(f\"Running for dataset config: {config}\")\n",
    "        \n",
    "        X_train = pd.read_csv(f\"{save_directory}/{dataset}_secondorder_{config}_X_train.csv\")\n",
    "        X_test = pd.read_csv(f\"{save_directory}/{dataset}_secondorder_{config}_X_test.csv\")\n",
    "        y_train = pd.read_csv(f\"{save_directory}/{dataset}_secondorder_{config}_y_train.csv\").values.ravel()\n",
    "        y_test = pd.read_csv(f\"{save_directory}/{dataset}_secondorder_{config}_y_test.csv\").values.ravel()\n",
    "        \n",
    "        print(X_train.shape)\n",
    "        \n",
    "        for ci_test in CI_tests:\n",
    "            print(ci_test.__class__.__name__)\n",
    "            for p_value in p_values:\n",
    "                \n",
    "                feature_gs, gs_time = run_with_timeout(gs_algorithm_running, X_train, X_test, y_train, y_test, ci_test, p_value)\n",
    "\n",
    "                \n",
    "                gs_results.append({\n",
    "                    'dataset': dataset,\n",
    "                    'config': config,\n",
    "                    'CI_test': ci_test.__class__.__name__,\n",
    "                    'p_value': p_value,\n",
    "                    'gs_features': feature_gs,\n",
    "                    'gs_time': gs_time,\n",
    "                    'n_features': len(feature_gs) if feature_gs != [\"max\"] else \"max\"\n",
    "                })\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(gs_results)\n",
    "\n",
    "\n",
    "results_df.to_csv('features_list_gs_regr.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9541a7b7",
   "metadata": {},
   "source": [
    "# Алгоритмы отбора признаков для сравнения"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ba9c5e",
   "metadata": {},
   "source": [
    "### Алгоритмы отбора признаков адаптированные под задачу классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15eac98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def algo_1 (X_train, X_test, y_train, y_test):\n",
    "\n",
    "    from sklearn.feature_selection import GenericUnivariateSelect, f_classif\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    \n",
    "    transformer = GenericUnivariateSelect(score_func=f_classif, mode='fpr', param=0.01)\n",
    "\n",
    "    \n",
    "    X_new = transformer.fit_transform(X_train, y_train)\n",
    "\n",
    "    \n",
    "    selected_features = X_train.columns[transformer.get_support()].tolist()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    elapsed_time = end_time - start_time\n",
    "    \n",
    "    return selected_features, elapsed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d522dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def algo_lasso (X_train, X_test, y_train, y_test):\n",
    "    start_time = time.time()\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "   \n",
    "    lasso = Lasso(alpha=0.1) \n",
    "    lasso.fit(X_scaled, y_train)\n",
    "\n",
    "    nonzero_coefficients = lasso.coef_ != 0\n",
    "    selected_features = X_train.columns[nonzero_coefficients].tolist()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    elapsed_time = end_time - start_time\n",
    "    \n",
    "    return selected_features, elapsed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb837545",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_f (X_train, X_test, y_train, y_test, n_features):\n",
    "    start_time = time.time()\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "     \n",
    "    le = LabelEncoder()\n",
    "    y_train = le.fit_transform(y_train)\n",
    "    \n",
    "    \n",
    "    unseen_labels = set(y_test) - set(le.classes_)\n",
    "    if unseen_labels:\n",
    "        mask = ~np.isin(y_test, list(unseen_labels))\n",
    "        X_test = X_test[mask]\n",
    "        y_test = y_test[mask]\n",
    "    \n",
    "    y_test = le.transform(y_test)\n",
    "    \n",
    "    model = xgb.XGBClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    importances = model.feature_importances_\n",
    "\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': X_train.columns,\n",
    "        'Importance': importances\n",
    "    })\n",
    "\n",
    "    importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "    importance_df=importance_df.head(n_features)\n",
    "    feature_list = importance_df['Feature'].tolist()\n",
    "\n",
    "    end_time = time.time()\n",
    "    \n",
    "    elapsed_time = end_time - start_time\n",
    "    \n",
    "    return feature_list, elapsed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599d3f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest (X_train, X_test, y_train, y_test, n_features):\n",
    "    start_time = time.time()\n",
    "\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    importances = model.feature_importances_\n",
    "\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': X_train.columns,\n",
    "        'Importance': importances\n",
    "    })\n",
    "\n",
    "    importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "    importance_df=importance_df.head(n_features)\n",
    "    feature_list = importance_df['Feature'].tolist()\n",
    "\n",
    "    end_time = time.time()\n",
    "    \n",
    "    elapsed_time = end_time - start_time\n",
    "    \n",
    "    return feature_list, elapsed_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d07c067",
   "metadata": {},
   "source": [
    "### Алгоритмы отбора признаков адаптированные под задачу регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762e21cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "def algo_1(X_train, X_test, y_train, y_test):\n",
    "    from sklearn.feature_selection import GenericUnivariateSelect, f_regression\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    \n",
    "    transformer = GenericUnivariateSelect(score_func=f_regression, mode='fpr', param=0.01)\n",
    "    \n",
    "    X_new = transformer.fit_transform(X_train, y_train)\n",
    "\n",
    "    selected_features = X_train.columns[transformer.get_support()].tolist()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    elapsed_time = end_time - start_time\n",
    "    \n",
    "    return selected_features, elapsed_time\n",
    "\n",
    "\n",
    "def algo_lasso(X_train, X_test, y_train, y_test):\n",
    "    start_time = time.time()\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "    lasso = Lasso(alpha=0.1) \n",
    "    lasso.fit(X_scaled, y_train)\n",
    "\n",
    "    nonzero_coefficients = lasso.coef_ != 0\n",
    "    selected_features = X_train.columns[nonzero_coefficients].tolist()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    elapsed_time = end_time - start_time\n",
    "    \n",
    "    return selected_features, elapsed_time\n",
    "\n",
    "def xgb_f(X_train, X_test, y_train, y_test, n_features):\n",
    "    start_time = time.time()\n",
    "\n",
    "    model = xgb.XGBRegressor()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    importances = model.feature_importances_\n",
    "\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': X_train.columns,\n",
    "        'Importance': importances\n",
    "    })\n",
    "\n",
    "    importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "    importance_df = importance_df.head(n_features)\n",
    "    feature_list = importance_df['Feature'].tolist()\n",
    "\n",
    "    end_time = time.time()\n",
    "    \n",
    "    elapsed_time = end_time - start_time\n",
    "    \n",
    "    return feature_list, elapsed_time\n",
    "\n",
    "def random_forest(X_train, X_test, y_train, y_test, n_features):\n",
    "    start_time = time.time()\n",
    "\n",
    "\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    importances = model.feature_importances_\n",
    "\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': X_train.columns,\n",
    "        'Importance': importances\n",
    "    })\n",
    "\n",
    "    importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "    importance_df = importance_df.head(n_features)\n",
    "    feature_list = importance_df['Feature'].tolist()\n",
    "\n",
    "    end_time = time.time()\n",
    "    \n",
    "    elapsed_time = end_time - start_time\n",
    "    \n",
    "    return feature_list, elapsed_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14a0ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\\\n",
    "gs_results_df = pd.read_csv('features_list_gs.csv')\n",
    "\n",
    "\n",
    "algo1_results = []\n",
    "lasso_results = []\n",
    "xgb_results = []\n",
    "rf_results = []\n",
    "\n",
    "\n",
    "for idx, row in gs_results_df.iterrows():\n",
    "    dataset = row['dataset']\n",
    "    config = row['config']\n",
    "    p_value = row['p_value']\n",
    "    \n",
    "    n_features = row['n_features']  \n",
    "    \n",
    "    \n",
    "    if n_features == 'max':\n",
    "        algo1_results.append({'dataset': dataset, 'config': config, 'selected_features': 0, 'time': 0})\n",
    "        lasso_results.append({'dataset': dataset, 'config': config, 'selected_features': 0, 'time': 0})\n",
    "        xgb_results.append({'dataset': dataset, 'config': config, 'selected_features': 0, 'time': 0})\n",
    "        rf_results.append({'dataset': dataset, 'config': config, 'selected_features': 0, 'time': 0})\n",
    "        continue  \n",
    "    \n",
    "    \n",
    "    n_features = int(n_features)\n",
    "    \n",
    "    if config == 0:\n",
    "        config = '0'\n",
    "    else:\n",
    "        config = f\"0{config:.0f}\"\n",
    "        print(config)\n",
    "    \n",
    "    \n",
    "    if p_value == 0.1:\n",
    "        print(f\"Running for dataset: {dataset}, config: {config}, p_value: {p_value}\")\n",
    "        \n",
    "        X_train = pd.read_csv(f\"{save_directory}/{dataset}_secondorder_{config}_X_train.csv\")\n",
    "        X_test = pd.read_csv(f\"{save_directory}/{dataset}_secondorder_{config}_X_test.csv\")\n",
    "        y_train = pd.read_csv(f\"{save_directory}/{dataset}_secondorder_{config}_y_train.csv\").values.ravel()\n",
    "        y_test = pd.read_csv(f\"{save_directory}/{dataset}_secondorder_{config}_y_test.csv\").values.ravel()\n",
    "        \n",
    "\n",
    "        selected_features_algo1, algo1_time = algo_1(X_train, X_test, y_train, y_test)\n",
    "        algo1_results.append({'dataset': dataset, 'config': config, 'selected_features': selected_features_algo1, 'time': algo1_time})\n",
    "\n",
    "        selected_features_lasso, lasso_time = algo_lasso(X_train, X_test, y_train, y_test)\n",
    "        lasso_results.append({'dataset': dataset, 'config': config, 'selected_features': selected_features_lasso, 'time': lasso_time})\n",
    "\n",
    "        selected_features_xgb, xgb_time = xgb_f(X_train, X_test, y_train, y_test, n_features)\n",
    "        xgb_results.append({'dataset': dataset, 'config': config, 'selected_features': selected_features_xgb, 'time': xgb_time})\n",
    "\n",
    "        selected_features_rf, rf_time = random_forest(X_train, X_test, y_train, y_test, n_features)\n",
    "        rf_results.append({'dataset': dataset, 'config': config, 'selected_features': selected_features_rf, 'time': rf_time})\n",
    "\n",
    "algo1_df = pd.DataFrame(algo1_results)\n",
    "lasso_df = pd.DataFrame(lasso_results)\n",
    "xgb_df = pd.DataFrame(xgb_results)\n",
    "rf_df = pd.DataFrame(rf_results)\n",
    "\n",
    "algo1_df.to_csv('algo1_results.csv', index=False)\n",
    "lasso_df.to_csv('lasso_results.csv', index=False)\n",
    "xgb_df.to_csv('xgb_results.csv', index=False)\n",
    "rf_df.to_csv('rf_results.csv', index=False)\n",
    "\n",
    "print(\"Feature selection results saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1279c2f",
   "metadata": {},
   "source": [
    "# Модели классификации для проверки качества"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb54f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "import lightgbm as lgb\n",
    "import time\n",
    "\n",
    "def train_mlp_classifier(X_train, X_test, y_train, y_test, feature_subset=None):\n",
    "    if feature_subset is not None:\n",
    "        X_train = X_train[feature_subset]\n",
    "        X_test = X_test[feature_subset]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    mlp = MLPClassifier(max_iter=200, random_state=42)\n",
    "    mlp.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = mlp.predict(X_test)\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "    end_time = time.time()\n",
    "    \n",
    "    elapsed_time = end_time - start_time\n",
    "    \n",
    "    return f1, elapsed_time\n",
    "\n",
    "\n",
    "def train_lgbm_classifier(X_train, X_test, y_train, y_test, feature_subset=None):\n",
    "    if feature_subset is not None:\n",
    "        X_train = X_train[feature_subset]\n",
    "        X_test = X_test[feature_subset]\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    lgbm = lgb.LGBMClassifier(max_iter=200, random_state=42, verbosity=-1)\n",
    "    lgbm.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = lgbm.predict(X_test)\n",
    "    f1 = f1_score(y_test, y_pred,  average='macro')\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    elapsed_time = end_time - start_time\n",
    "    \n",
    "    return f1, elapsed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124cc359",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "gs_results_df = pd.read_csv('features_list_gs.csv')\n",
    "\n",
    "\n",
    "mlp_f1_scores = []\n",
    "mlp_times = []\n",
    "lgbm_f1_scores = []\n",
    "lgbm_times = []\n",
    "\n",
    "for idx, row in gs_results_df.iterrows():\n",
    "    dataset = row['dataset']\n",
    "    config = row['config']\n",
    "    gs_features = row['gs_features']  \n",
    "    \n",
    "\n",
    "    if config == 0:\n",
    "        config_str = '0'\n",
    "    else:\n",
    "        config_str = f\"0{config:.0f}\"\n",
    "\n",
    "\n",
    "    X_train = pd.read_csv(f\"{save_directory}/{dataset}_secondorder_{config_str}_X_train.csv\")\n",
    "    X_test = pd.read_csv(f\"{save_directory}/{dataset}_secondorder_{config_str}_X_test.csv\")\n",
    "    \n",
    "\n",
    "    y_train = pd.read_csv(f\"{save_directory}/{dataset}_secondorder_{config_str}_y_train.csv\").values.ravel()\n",
    "    y_test = pd.read_csv(f\"{save_directory}/{dataset}_secondorder_{config_str}_y_test.csv\").values.ravel()\n",
    "\n",
    "\n",
    "    if gs_features == \"[]\" or gs_features == \"max\":\n",
    "        selected_features = None  \n",
    "    else:\n",
    "        selected_features = gs_features.strip('[]').replace(\"'\", \"\").split(', ')\n",
    "\n",
    "\n",
    "    if selected_features is not None:\n",
    "        X_train_selected = X_train[selected_features]\n",
    "        X_test_selected = X_test[selected_features]\n",
    "    else:\n",
    "        X_train_selected = X_train\n",
    "        X_test_selected = X_test\n",
    "        \n",
    "    print(selected_features)\n",
    "\n",
    "\n",
    "    mlp_f1, mlp_time = train_mlp_regressor(X_train_selected, X_test_selected, y_train, y_test)\n",
    "    lgbm_f1, lgbm_time = train_lgbm_regressor(X_train_selected, X_test_selected, y_train, y_test)\n",
    "\n",
    "\n",
    "    mlp_f1_scores.append(mlp_f1)\n",
    "    mlp_times.append(mlp_time)\n",
    "    lgbm_f1_scores.append(lgbm_f1)\n",
    "    lgbm_times.append(lgbm_time)\n",
    "\n",
    "gs_results_df['mlp_f1'] = mlp_f1_scores\n",
    "gs_results_df['mlp_time'] = mlp_times\n",
    "gs_results_df['lgbm_f1'] = lgbm_f1_scores\n",
    "gs_results_df['lgbm_time'] = lgbm_times\n",
    "\n",
    "\n",
    "gs_results_df.to_csv('features_list_gs_with_configs_and_f1.csv', index=False)\n",
    "\n",
    "print(\"F1 scores and times added to the dataset and saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7596d93",
   "metadata": {},
   "source": [
    "# Модели регрессии для оценки качества"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc34837",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "def train_mlp_regressor(X_train, X_test, y_train, y_test, feature_subset=None):\n",
    "    if feature_subset is not None:\n",
    "        X_train = X_train[feature_subset]\n",
    "        X_test = X_test[feature_subset]\n",
    "\n",
    "\n",
    "    param_grid = {\n",
    "        'alpha': [ 0.001, 0.01],\n",
    "        'learning_rate_init': [ 0.01, 0.1],\n",
    "        'max_iter': [1000]  \n",
    "    }\n",
    "\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    mlp = MLPRegressor(random_state=42)\n",
    "    grid_search = GridSearchCV(mlp, param_grid, cv=3, scoring='neg_mean_squared_error')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    best_mlp = grid_search.best_estimator_\n",
    "\n",
    "    y_pred = best_mlp.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    elapsed_time = end_time - start_time\n",
    "    \n",
    "    print(f\"MLP RMSE: {rmse}\")\n",
    "    \n",
    "    return round(rmse, 2), round(elapsed_time, 2)\n",
    "\n",
    "\n",
    "def train_lgbm_regressor(X_train, X_test, y_train, y_test, feature_subset=None):\n",
    "    if feature_subset is not None:\n",
    "        X_train = X_train[feature_subset]\n",
    "        X_test = X_test[feature_subset]\n",
    "\n",
    "    param_grid = {\n",
    "        'n_estimators': [500],\n",
    "        'learning_rate': [ 0.05, 0.1],\n",
    "        'num_leaves': [ 100]\n",
    "    }\n",
    "\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    lgbm = lgb.LGBMRegressor(random_state=42, verbosity=-1)\n",
    "    grid_search = GridSearchCV(lgbm, param_grid, cv=3, scoring='neg_mean_squared_error')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    best_lgbm = grid_search.best_estimator_\n",
    "\n",
    "    y_pred = best_lgbm.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    elapsed_time = end_time - start_time\n",
    "    \n",
    "    print(f\"LGBM RMSE: {rmse}\")\n",
    "    \n",
    "    return round(rmse, 2), round(elapsed_time, 2)\n",
    "\n",
    "\n",
    "def train_xgb_regressor(X_train, X_test, y_train, y_test, feature_subset=None):\n",
    "    if feature_subset is not None:\n",
    "        X_train = X_train[feature_subset]\n",
    "        X_test = X_test[feature_subset]\n",
    "    \n",
    "\n",
    "    param_grid = {\n",
    "        'n_estimators': [500],\n",
    "        'learning_rate': [ 0.05, 0.1],\n",
    "        'max_depth': [6]\n",
    "    }\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    xg_reg = xgb.XGBRegressor(objective='reg:squarederror', random_state=42, verbosity=0)\n",
    "    grid_search = GridSearchCV(xg_reg, param_grid, cv=3, scoring='neg_mean_squared_error')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    best_xg = grid_search.best_estimator_\n",
    "\n",
    "    y_pred = best_xg.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    elapsed_time = end_time - start_time\n",
    "    \n",
    "    print(f\"XGBoost RMSE: {rmse}\")\n",
    "    \n",
    "    return round(rmse, 2), round(elapsed_time, 2)\n",
    "\n",
    "\n",
    "def train_linear_regressor(X_train, X_test, y_train, y_test, feature_subset=None):\n",
    "    if feature_subset is not None:\n",
    "        X_train = X_train[feature_subset]\n",
    "        X_test = X_test[feature_subset]\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "\n",
    "    lin_reg = Ridge(alpha=1.0)  \n",
    "    lin_reg.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = lin_reg.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    elapsed_time = end_time - start_time\n",
    "    \n",
    "    print(f\"Linear Regression RMSE: {rmse}\")\n",
    "    \n",
    "    return round(rmse, 2), round(elapsed_time, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e67873e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "dataset_configs = ['0', '01', '02', '03']\n",
    "\n",
    "mlp_rmse_scores = []\n",
    "mlp_times = []\n",
    "lgbm_rmse_scores = []\n",
    "lgbm_times = []\n",
    "xgb_rmse_scores = []\n",
    "xgb_times = []\n",
    "linreg_rmse_scores = []\n",
    "linreg_times = []\n",
    "datasets = []\n",
    "configs = []\n",
    "\n",
    "for dataset in list_df_regr:\n",
    "    for config in dataset_configs:\n",
    "        \n",
    "        X_train = pd.read_csv(f\"{save_directory}/{dataset}_secondorder_{config}_X_train.csv\")\n",
    "        X_test = pd.read_csv(f\"{save_directory}/{dataset}_secondorder_{config}_X_test.csv\")\n",
    "        y_train = pd.read_csv(f\"{save_directory}/{dataset}_secondorder_{config}_y_train.csv\").values.ravel()\n",
    "        y_test = pd.read_csv(f\"{save_directory}/{dataset}_secondorder_{config}_y_test.csv\").values.ravel()\n",
    "\n",
    "        \n",
    "        mlp_rmse, mlp_time = train_mlp_regressor(X_train, X_test, y_train, y_test)\n",
    "        \n",
    "        \n",
    "        lgbm_rmse, lgbm_time = train_lgbm_regressor(X_train, X_test, y_train, y_test)\n",
    "\n",
    "        \n",
    "        xgb_rmse, xgb_time = train_xgb_regressor(X_train, X_test, y_train, y_test)\n",
    "\n",
    "        \n",
    "        linreg_rmse, linreg_time = train_linear_regressor(X_train, X_test, y_train, y_test)\n",
    "\n",
    "        \n",
    "        datasets.append(dataset)\n",
    "        configs.append(config)\n",
    "        mlp_rmse_scores.append(mlp_rmse)\n",
    "        mlp_times.append(mlp_time)\n",
    "        lgbm_rmse_scores.append(lgbm_rmse)\n",
    "        lgbm_times.append(lgbm_time)\n",
    "        xgb_rmse_scores.append(xgb_rmse)\n",
    "        xgb_times.append(xgb_time)\n",
    "        linreg_rmse_scores.append(linreg_rmse)\n",
    "        linreg_times.append(linreg_time)\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'dataset': datasets,\n",
    "    'config': configs,\n",
    "    'mlp_rmse': mlp_rmse_scores,\n",
    "    'mlp_time': mlp_times,\n",
    "    'lgbm_rmse': lgbm_rmse_scores,\n",
    "    'lgbm_time': lgbm_times,\n",
    "    'xgb_rmse': xgb_rmse_scores,\n",
    "    'xgb_time': xgb_times,\n",
    "    'linreg_rmse': linreg_rmse_scores,\n",
    "    'linreg_time': linreg_times\n",
    "})\n",
    "\n",
    "\n",
    "results_df.to_csv('mlp_lgbm_xgb_linreg_rmse_results_regr_with_configs.csv', index=False)\n",
    "\n",
    "print(\"RMSE scores and times saved for MLP, LGBM, XGBoost, and Linear Regression for all datasets and configurations.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87033bde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
